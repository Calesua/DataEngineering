{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68728c61-aa32-4564-b3ee-06ae9d47a254",
   "metadata": {},
   "source": [
    "# Práctica 1 Hadoop - MapReduce + Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80460439-b303-486a-8b3d-088a75f6e112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/practica1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# nos vamos a esa carpeta y comprobamos donde estamos\n",
    "os.chdir(\"/media/notebooks/practica1\")\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28e9ad-644f-4d2f-acb8-86b9f1cb8b79",
   "metadata": {},
   "source": [
    "Creamos una carpeta para los ejercicios en hdfs y subimos los ficheros de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9375d89c-cffa-4cd1-8596-ec0ac3fad820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/practica1/ejercicio1': File exists\n",
      "Found 5 items\n",
      "-rw-r--r--   3 root supergroup          0 2024-11-20 17:10 /practica1/archivo_seleccionado.csv\n",
      "-rw-r--r--   3 root supergroup       1289 2024-11-20 17:10 /practica1/clients.csv\n",
      "-rw-r--r--   3 root supergroup          0 2024-11-20 17:10 /practica1/convocatorias-2020.csv\n",
      "-rw-r--r--   3 root supergroup       4120 2024-11-20 17:10 /practica1/countries.csv\n",
      "drwxr-xr-x   - root supergroup          0 2024-11-20 17:09 /practica1/ejercicio1\n"
     ]
    }
   ],
   "source": [
    "#! hdfs dfs -mkdir /practica1\n",
    "! hdfs dfs -mkdir /practica1/ejercicio1\n",
    "! hdfs dfs -put /media/notebooks/practica1/*.csv /practica1\n",
    "! hdfs dfs -ls /practica1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "963921a7-b04e-4171-ac9c-b5ca3835c2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertram Pearcy  ,bueno,SO\n",
      "Steven Ulman  ,regular,ZA\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /practica1/ejercicio1/clients.csv | head -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "59901b48-0e49-474f-bb69-7ef0053b2a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/51-0159-LPR19,381,381 - Estado Mayor General de La Fuerza Aérea,40,Departamento Contrataciones Córdoba - UOC 40/51,Licitacion Privada,Sin Modalidad,,2019,02/01/2020 08:00:00 a.m.,10/01/2020 10:00:00 a.m.,Única,Nacional,“Adquisición de Elementos de Librería para el HOSPITAL AERONÁUTICO CÓRDOBA,“Adquisición de Elementos de Librería para el HOSPITAL AERONÁUTICO CÓRDOBA,1957142.84,Proceso de Compra\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /practica1/ejercicio1/convocatorias-2020.csv | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "15ba221e-6bde-4709-886b-f1cc903d268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name,Code\n",
      "Afghanistan,AF\n",
      "Åland Islands,AX\n"
     ]
    }
   ],
   "source": [
    "# Mapeo código-nombre de pais\n",
    "! hdfs dfs -cat /practica1/ejercicio1/countries.csv | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb7836-23b8-4115-a896-edb2d447820f",
   "metadata": {},
   "source": [
    "## Ejercicio 1.1: Contador de clientes valorados por países.\n",
    "Cuántos clientes con valoración “bueno” hay en cada país."
   ]
  },
  {
   "attachments": {
    "042cae48-3961-4b7e-8509-55ebfd881ca8.png": {
     "image/png": "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"
    },
    "f65b82f9-a0e4-4ae9-ac89-9e67349eb29e.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "017594e2-ca86-4a74-ab35-ae493cfaa2fd",
   "metadata": {},
   "source": [
    "**Diseño del programa**\n",
    "\n",
    "Como mínimo sería necesario incluir los pasos de map y de reduce. Sin embargo, sería posible optimizar el proceso añadiendo también un combiner que traslade parte del trabajo de reducción a los nodos. Esto sería especialmente útil con cargas mayores de trabajo, pero lo vamos a aplicar en este caso.\n",
    "\n",
    "Es necesario hacer un proceso de mapeo de los códigos de país con los nombres de los países. Para esto se puede considerar varias opciones, la primera es distribuir el archivo countries.csv en el cluster y aplicarle un mapper especializado en paralelo al mapper que ya le estaríamos aplicando a los datos de trabajo. Esta sería la opción ideal si el archivo de mapeo es muy grande. Otra opción, quizás la óptima, sería aplicar el mapeo de países al final del proceso, después de la reducción. Sin embargo, el ejercicio pide que la salida del proceso map-reduce presente los nombres de los países y no sus códigos. \n",
    "\n",
    "Por este motivo, en este trabajo lo que se hará es aplicar dicho mapeo dentro del proceso de reducción, distribuyendo previamente el archivo countries.csv en \"caché distribuído\". El caché distribuído guarda una copia del archivo afectado en cada uno de los nodos que participan en el proceso MapReduce. De esta manera cada nodo reducer dispondrá de una copia local del archivo para realizar la conversión de código a nombre de país. Esta opción es recomendable únicamente cuando el archivo cargado en memoria es pequeño, como es este caso. La elección de aplicar la conversión de codigos en la etapa reduce se debe a que, en general, habrá menor cantidad de nodos implicados en este proceso que en el de mapping. Además, los datos habrán sido reducidos al agregarse los datos intermedios. Ambos hechos hacen que la cantidad de datos que pasan por el proceso de conversión sea menor y se mejore la eficiencia.\n",
    "\n",
    "Resumidamente, el flujo de datos es el siguiente:\n",
    "- La entrada del mapper es cada línea del archivo clients\n",
    "![Img1.png](attachment:042cae48-3961-4b7e-8509-55ebfd881ca8.png)\n",
    "\n",
    "- El mapper filtra en cada datanode (sobre la porción de datos que contiene) las líneas con opinión \"bueno\" y devuelve pares clave-valor de la forma (código de país)-(contador con el número 1)\n",
    "\n",
    " ![Img2.png](attachment:f65b82f9-a0e4-4ae9-ac89-9e67349eb29e.png)\n",
    "\n",
    "- El combiner en cada datanode recibe estas líneas y agrega las cantidades en el contador por cada código de país distinto, devolviendo pares (código de país)-(contador agregado)\n",
    "\n",
    "- Los datos procedentes de los combiners en los datanodes son distribuidos en los diferentes reducers mediante el proceso de shuffle & sort ordenados y agrupados en función de sus claves.\n",
    "\n",
    "- Finalmente, en los nodos de agregación se reciben estos pares clave-valor intermedios, donde se vuelven a agregar las cantidades de los contadores, se realiza la \"traducción\" de los códigos de país mediante un diccionario y se devuelve los nuevos pares (país)-(contador agregado) ordenados por país."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e04f9-90a0-4d24-aaf0-8a9719f26425",
   "metadata": {},
   "source": [
    "Creamos scripts con los procesos de map, combine y reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d868a4a8-66a9-4aa1-a422-1f09d8a28617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 11_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 11_mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Este mapper devuelve por cada línea con opinión positiva, el código de país y un 1\n",
    "\n",
    "import sys\n",
    "\n",
    "# entrada estandar STDIN\n",
    "for linea in sys.stdin:\n",
    "    \n",
    "    # eliminamos espacios y dividimos cada linea en palabras\n",
    "    campos = linea.strip().split(',')\n",
    "    opinion, pais = campos[1], campos[2]\n",
    "\n",
    "    if opinion == 'bueno':\n",
    "        # cada palabra la escribimos junto con un 1 y esta es la entrada del combiner\n",
    "        print ('%s\\t%s' % (pais, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "07b607d8-3060-499b-b723-4c0877532cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 11_combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 11_combiner.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "# Ulizamos un diccionario para las palabras con un entero como valor\n",
    "contador_por_pais = defaultdict(int)\n",
    "\n",
    "# Sumamos las apariciones parciales (en este caso 1) al valor de cada clave (país) cuando esta aparece en una línea\n",
    "for linea in sys.stdin:\n",
    "    codigo, numero = linea.strip().split('\\t')\n",
    "    contador_por_pais[codigo] += int(numero)\n",
    "\n",
    "# Salida: por cada pais el numero total de apariciones\n",
    "for codigo in contador_por_pais.keys():\n",
    "    print ('%s\\t%s' % (codigo, contador_por_pais[codigo]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0881724c-96c6-4dc2-bffd-96563a28e231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 11_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 11_reducer.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "# 1. Carga del mapeo codigo:pais:\n",
    "def carga_countries(file):\n",
    "    mapping = {}\n",
    "    with open(file, 'r') as f:\n",
    "        next(f)  # Avanza una línea para saltar el header\n",
    "        for line in f:\n",
    "            campos = line.strip().split(',')\n",
    "            pais, codigo = (\",\".join(campos[:-1]), campos[-1]) # Unimos los campos separados por la coma del nombre de algunos paises\n",
    "            mapping[codigo] = pais\n",
    "    return mapping\n",
    "\n",
    "\n",
    "# 2. Reducer\n",
    "mapping = carga_countries(\"countries.csv\")\n",
    "\n",
    "# Ulizamos un diccionario para las palabras con un entero como valor\n",
    "contador_por_pais = defaultdict(int)\n",
    "\n",
    "# Sumamos las apariciones parciales (en este caso 1) al valor de cada clave (país) cuando esta aparece en una línea\n",
    "for linea in sys.stdin:\n",
    "    codigo, numero = linea.strip().split('\\t')\n",
    "    contador_por_pais[codigo] += int(numero)\n",
    "\n",
    "# Mapeamos los codigos con los nombres de los paises\n",
    "convert_contador = {mapping[codigo]: contador_por_pais[codigo] for codigo in contador_por_pais.keys()}\n",
    "\n",
    "# escribimos por cada pais el numero total de apariciones\n",
    "for pais, numero in sorted(convert_contador.items(), key=lambda item: item[0]):\n",
    "    print ('%s\\t%s' % (pais, numero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e3686951-4650-454e-b028-5b8957ddde98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar9130850184786831685/] [] /tmp/streamjob2890519559786905807.jar tmpDir=null\n",
      "2024-11-15 20:15:04,641 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.29.0.3:8032\n",
      "2024-11-15 20:15:04,779 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.29.0.3:8032\n",
      "2024-11-15 20:15:05,026 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1731686121000_0021\n",
      "2024-11-15 20:15:05,542 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-11-15 20:15:05,610 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-11-15 20:15:05,696 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1731686121000_0021\n",
      "2024-11-15 20:15:05,696 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-11-15 20:15:05,861 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-11-15 20:15:05,861 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-11-15 20:15:05,925 INFO impl.YarnClientImpl: Submitted application application_1731686121000_0021\n",
      "2024-11-15 20:15:05,956 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1731686121000_0021/\n",
      "2024-11-15 20:15:05,958 INFO mapreduce.Job: Running job: job_1731686121000_0021\n",
      "2024-11-15 20:15:11,130 INFO mapreduce.Job: Job job_1731686121000_0021 running in uber mode : false\n",
      "2024-11-15 20:15:11,131 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-11-15 20:15:16,196 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-11-15 20:15:20,222 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-11-15 20:15:21,241 INFO mapreduce.Job: Job job_1731686121000_0021 completed successfully\n",
      "2024-11-15 20:15:21,348 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=104\n",
      "\t\tFILE: Number of bytes written=945567\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2122\n",
      "\t\tHDFS: Number of bytes written=163\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4925\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1986\n",
      "\t\tTotal time spent by all map tasks (ms)=4925\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1986\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4925\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1986\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5043200\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2033664\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=50\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=80\n",
      "\t\tMap output materialized bytes=110\n",
      "\t\tInput split bytes=188\n",
      "\t\tCombine input records=16\n",
      "\t\tCombine output records=14\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce shuffle bytes=110\n",
      "\t\tReduce input records=14\n",
      "\t\tReduce output records=12\n",
      "\t\tSpilled Records=28\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=172\n",
      "\t\tCPU time spent (ms)=2730\n",
      "\t\tPhysical memory (bytes) snapshot=887775232\n",
      "\t\tVirtual memory (bytes) snapshot=7803412480\n",
      "\t\tTotal committed heap usage (bytes)=764411904\n",
      "\t\tPeak Map Physical memory (bytes)=319225856\n",
      "\t\tPeak Map Virtual memory (bytes)=2599976960\n",
      "\t\tPeak Reduce Physical memory (bytes)=255066112\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2604175360\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1934\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=163\n",
      "2024-11-15 20:15:21,348 INFO streaming.StreamJob: Output directory: /practica1/11_salida\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-files 11_mapper.py,11_combiner.py,11_reducer.py,countries.csv \\\n",
    "-mapper 11_mapper.py \\\n",
    "-combiner 11_combiner.py \\\n",
    "-reducer 11_reducer.py \\\n",
    "-input /practica1/clients.csv \\\n",
    "-output /practica1/11_salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4cc9b0-176c-4886-a550-27b08f4282d5",
   "metadata": {},
   "source": [
    "Ejecutamos el trabajo MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c5dcdee5-f645-4880-bdd5-7de0a441c3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar6951514998334055932/] [] /tmp/streamjob6969003701553846985.jar tmpDir=null\n",
      "2024-11-16 09:28:20,488 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.29.0.3:8032\n",
      "2024-11-16 09:28:20,671 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.29.0.3:8032\n",
      "2024-11-16 09:28:20,896 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1731686121000_0022\n",
      "2024-11-16 09:28:21,769 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-11-16 09:28:22,091 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-11-16 09:28:22,322 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1731686121000_0022\n",
      "2024-11-16 09:28:22,322 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-11-16 09:28:22,481 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-11-16 09:28:22,481 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-11-16 09:28:22,618 INFO impl.YarnClientImpl: Submitted application application_1731686121000_0022\n",
      "2024-11-16 09:28:22,677 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1731686121000_0022/\n",
      "2024-11-16 09:28:22,683 INFO mapreduce.Job: Running job: job_1731686121000_0022\n",
      "2024-11-16 09:28:29,838 INFO mapreduce.Job: Job job_1731686121000_0022 running in uber mode : false\n",
      "2024-11-16 09:28:29,839 INFO mapreduce.Job:  map 0% reduce 0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 09:28:47,625 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-11-16 09:28:52,652 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-11-16 09:28:52,659 INFO mapreduce.Job: Job job_1731686121000_0022 completed successfully\n",
      "2024-11-16 09:28:52,787 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=104\n",
      "\t\tFILE: Number of bytes written=945567\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2122\n",
      "\t\tHDFS: Number of bytes written=163\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=29328\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2615\n",
      "\t\tTotal time spent by all map tasks (ms)=29328\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2615\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=29328\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2615\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=30031872\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2677760\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=50\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=80\n",
      "\t\tMap output materialized bytes=110\n",
      "\t\tInput split bytes=188\n",
      "\t\tCombine input records=16\n",
      "\t\tCombine output records=14\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce shuffle bytes=110\n",
      "\t\tReduce input records=14\n",
      "\t\tReduce output records=12\n",
      "\t\tSpilled Records=28\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=155\n",
      "\t\tCPU time spent (ms)=4580\n",
      "\t\tPhysical memory (bytes) snapshot=903786496\n",
      "\t\tVirtual memory (bytes) snapshot=7804936192\n",
      "\t\tTotal committed heap usage (bytes)=771751936\n",
      "\t\tPeak Map Physical memory (bytes)=332210176\n",
      "\t\tPeak Map Virtual memory (bytes)=2600779776\n",
      "\t\tPeak Reduce Physical memory (bytes)=245436416\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2606993408\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1934\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=163\n",
      "2024-11-16 09:28:52,787 INFO streaming.StreamJob: Output directory: /practica1/112_salida\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-D mapreduce.output.fileoutputformat.compress=false \\\n",
    "-files 11_mapper.py,11_combiner.py,11_reducer.py,countries.csv \\\n",
    "-mapper 11_mapper.py \\\n",
    "-combiner 11_combiner.py \\\n",
    "-reducer 11_reducer.py \\\n",
    "-input /practica1/ejercicio1/clients.csv \\\n",
    "-output /practica1/ejercicio1/112_salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e2a8fd-5afa-4d49-a8f7-2e7c6d8265f5",
   "metadata": {},
   "source": [
    "En este caso el número de nodos tanto de mapeo como de reducción es de 1, con lo que esto no produciría una cambio significativo en el tiempo de traducción de los códigos de país."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7ab12-031f-48e4-87af-539e982b132e",
   "metadata": {},
   "source": [
    "Veamos el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "385a539d-5410-404a-bf9f-2fae86b49fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada\t1\n",
      "Guam\t3\n",
      "Guinea\t1\n",
      "Portugal\t1\n",
      "Qatar\t1\n",
      "Somalia\t1\n",
      "South Africa\t1\n",
      "South Georgia and the South Sandwich Islands\t1\n",
      "South Sudan\t1\n",
      "Spain\t3\n",
      "Turkey\t1\n",
      "United States\t1\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -tail /practica1/ejercicio1/11_salida/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc3988-47e5-4074-8ea3-60e87d12e8df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ejercicio 1.2: País con mejores clientes\n",
    "El objetivo es determinar el país en el que hay más clientes valorados como “bueno”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bee02-bb3b-4c4a-a6bc-eae6bf93c8bb",
   "metadata": {},
   "source": [
    "**Diseño del programa**\n",
    "\n",
    "En este caso bastaría con modificar el job de reducción para buscar el valor máximo de valoraciones buenas. \n",
    "\n",
    "Los pasos del proceso MapReduce serían los mismos que en apartado 1 y el flujo de datos sería el siguiente\n",
    "\n",
    "- La entrada del mapper es cada línea del archivo clients\n",
    "\n",
    "- El mapper filtra en cada datanode (sobre la porción de datos que contiene) las líneas con opinión \"bueno\" y devuelve pares clave-valor de la forma (código de país)-(contador con el número 1)\n",
    "\n",
    "- El combiner en cada datanode recibe estas líneas y agrega las cantidades en el contador por cada código de país distinto, devolviendo pares (código de país)-(contador agregado)\n",
    "\n",
    "- Los datos procedentes de los combiners en los datanodes son distribuidos en los diferentes reducers mediante el proceso de shuffle & sort ordenados y agrupados en función de sus claves.\n",
    "\n",
    "- Finalmente, en los nodos de agregación se reciben estos pares clave-valor intermedios, donde se vuelven a agregar las cantidades de los contadores, se realiza la \"traducción\" de los códigos de país y se busca entre los items del diccionario que contiene el número de valoraciones buenas por país aquel con mayor número. La salida es el par (país)-(contador agregado) correspondiente a dicho item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7d6c4577-5991-43c0-943f-7d7ecb71ba2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 12_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 12_reducer.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "# 1. Carga del mapeo codigo:pais:\n",
    "def carga_countries(file):\n",
    "    mapping = {}\n",
    "    with open(file, 'r') as f:\n",
    "        next(f)  # Avanza una línea para saltar el header\n",
    "        for line in f:\n",
    "            campos = line.strip().split(',')\n",
    "            pais, codigo = (\",\".join(campos[:-1]), campos[-1]) # Unimos los campos separados por la coma del nombre de algunos paises\n",
    "            mapping[codigo] = pais\n",
    "    return mapping\n",
    "\n",
    "\n",
    "# 2. Reducer\n",
    "mapping = carga_countries(\"countries.csv\")\n",
    "\n",
    "# Ulizamos un diccionario para las palabras con un entero como valor\n",
    "contador_por_pais = defaultdict(int)\n",
    "\n",
    "# Sumamos las apariciones parciales (en este caso 1) al valor de cada clave (país) cuando esta aparece en una línea\n",
    "for linea in sys.stdin:\n",
    "    codigo, numero = linea.strip().split('\\t')\n",
    "    contador_por_pais[codigo] += int(numero)\n",
    "\n",
    "# Mapeamos los codigos con los nombres de los paises\n",
    "convert_contador = {mapping[codigo]: contador_por_pais[codigo] for codigo in contador_por_pais.keys()}\n",
    "\n",
    "# Calculamos el máximo de valoraciones \"buenas\"\n",
    "pais, max_num = max(convert_contador.items(), key=lambda x: x[1])\n",
    "\n",
    "# Salida\n",
    "print ('%s\\t%s' % (max_num, pais))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a8e70-66a9-487b-a4ec-ced6c577bc4e",
   "metadata": {},
   "source": [
    "Ejecutamos el proceso MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49d4d022-3aa9-4bbc-b92c-a04cea2b05d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar6234014231424442825/] [] /tmp/streamjob1808462849641502438.jar tmpDir=null\n",
      "2024-11-15 17:41:17,563 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.29.0.3:8032\n",
      "2024-11-15 17:41:17,721 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.29.0.3:8032\n",
      "2024-11-15 17:41:17,941 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1731686121000_0008\n",
      "2024-11-15 17:41:18,392 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-11-15 17:41:18,456 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-11-15 17:41:18,546 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1731686121000_0008\n",
      "2024-11-15 17:41:18,546 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-11-15 17:41:18,697 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-11-15 17:41:18,698 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-11-15 17:41:18,763 INFO impl.YarnClientImpl: Submitted application application_1731686121000_0008\n",
      "2024-11-15 17:41:18,796 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1731686121000_0008/\n",
      "2024-11-15 17:41:18,797 INFO mapreduce.Job: Running job: job_1731686121000_0008\n",
      "2024-11-15 17:41:23,894 INFO mapreduce.Job: Job job_1731686121000_0008 running in uber mode : false\n",
      "2024-11-15 17:41:23,896 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-11-15 17:41:27,968 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-11-15 17:41:33,001 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-11-15 17:41:33,017 INFO mapreduce.Job: Job job_1731686121000_0008 completed successfully\n",
      "2024-11-15 17:41:33,109 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=104\n",
      "\t\tFILE: Number of bytes written=945567\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2122\n",
      "\t\tHDFS: Number of bytes written=12\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3907\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1741\n",
      "\t\tTotal time spent by all map tasks (ms)=3907\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1741\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3907\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1741\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4000768\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1782784\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=50\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=80\n",
      "\t\tMap output materialized bytes=110\n",
      "\t\tInput split bytes=188\n",
      "\t\tCombine input records=16\n",
      "\t\tCombine output records=14\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce shuffle bytes=110\n",
      "\t\tReduce input records=14\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=28\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=117\n",
      "\t\tCPU time spent (ms)=1810\n",
      "\t\tPhysical memory (bytes) snapshot=993419264\n",
      "\t\tVirtual memory (bytes) snapshot=7806365696\n",
      "\t\tTotal committed heap usage (bytes)=771227648\n",
      "\t\tPeak Map Physical memory (bytes)=365330432\n",
      "\t\tPeak Map Virtual memory (bytes)=2600919040\n",
      "\t\tPeak Reduce Physical memory (bytes)=263413760\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2605875200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1934\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=12\n",
      "2024-11-15 17:41:33,109 INFO streaming.StreamJob: Output directory: /practica1/12_salida\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-files 11_mapper.py,11_combiner.py,12_reducer.py,countries.csv \\\n",
    "-mapper 11_mapper.py \\\n",
    "-combiner 11_combiner.py \\\n",
    "-reducer 12_reducer.py \\\n",
    "-input /practica1/ejercicio1/clients.csv \\\n",
    "-output /practica1/ejercicio1/12_salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72304cfc-5d3c-4678-89ff-2c13cbe65071",
   "metadata": {},
   "source": [
    "Y obtenemos el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e7a21eeb-9874-42c0-b350-ce4a605b2875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t['Spain']\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -tail /practica1/ejercicio1/12_salida/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44c39d-c866-4133-9f97-c571931a6ebc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ejercicio 1.3: Mejorando el país con mejores clientes\n",
    "\n",
    "Devolver todos los países empatados con el mayor número de buenos clientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c203699a-658d-4445-af97-eceb58b508e3",
   "metadata": {},
   "source": [
    "**Diseño del programa**\n",
    "\n",
    "De nuevo, hay que modificar el job de reducción para que devuelva todos los items del diccionario con el número máximo de valoraciones buenas. \n",
    "\n",
    "Los pasos del proceso MapReduce serían los mismos que en apartado 1 y el flujo de datos sería el siguiente\n",
    "\n",
    "- La entrada del mapper es cada línea del archivo clients\n",
    "\n",
    "- El mapper filtra en cada datanode (sobre la porción de datos que contiene) las líneas con opinión \"bueno\" y devuelve pares clave-valor de la forma (código de país)-(contador con el número 1)\n",
    "\n",
    "- El combiner en cada datanode recibe estas líneas y agrega las cantidades en el contador por cada código de país distinto, devolviendo pares (código de país)-(contador agregado)\n",
    "\n",
    "- Los datos procedentes de los combiners en los datanodes son distribuidos en los diferentes reducers mediante el proceso de shuffle & sort ordenados y agrupados en función de sus claves.\n",
    "\n",
    "- Finalmente, en los nodos de agregación se reciben estos pares clave-valor intermedios, donde se vuelven a agregar las cantidades de los contadores y se realiza la \"traducción\" de los códigos de país. Luego se busca entre los items del diccionario que contiene el número de valoraciones buenas por país aquel con mayor número y se busca en el mismo diccionario todos los items con ese mismo valor. Se devuelve todos los pares (país)-(contador agregado) encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d3ea064e-c268-4525-8f61-8d1244df04db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 13_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 13_reducer.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "# 1. Carga del mapeo codigo:pais:\n",
    "def carga_countries(file):\n",
    "    mapping = {}\n",
    "    with open(file, 'r') as f:\n",
    "        next(f)  # Avanza una línea para saltar el header\n",
    "        for line in f:\n",
    "            campos = line.strip().split(',')\n",
    "            pais, codigo = (\",\".join(campos[:-1]), campos[-1]) # Unimos los campos separados por la coma del nombre de algunos paises\n",
    "            mapping[codigo] = pais\n",
    "    return mapping\n",
    "\n",
    "\n",
    "# 2. Reducer\n",
    "mapping = carga_countries(\"countries.csv\")\n",
    "\n",
    "# Ulizamos un diccionario para las palabras con un entero como valor\n",
    "contador_por_pais = defaultdict(int)\n",
    "\n",
    "# Sumamos las apariciones parciales (en este caso 1) al valor de cada clave (país) cuando esta aparece en una línea\n",
    "for linea in sys.stdin:\n",
    "    codigo, numero = linea.strip().split('\\t')\n",
    "    contador_por_pais[codigo] += int(numero)\n",
    "\n",
    "# Mapeamos los codigos con los nombres de los paises\n",
    "convert_contador = {mapping[codigo]: contador_por_pais[codigo] for codigo in contador_por_pais.keys()}\n",
    "\n",
    "# Calculamos el máximo de opiniones \"buenas\"\n",
    "pais, max_num = max(convert_contador.items(), key=lambda x: x[1])\n",
    "for pais, num in sorted(convert_contador.items(), key=lambda item: item[0]):\n",
    "    if num == max_num:\n",
    "        print ('%s\\t%s' % (num, pais))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4fb50c61-1e47-437f-b694-aa45fc7a809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar8180910531056673924/] [] /tmp/streamjob2273448452937181370.jar tmpDir=null\n",
      "2024-11-15 19:05:52,029 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.29.0.3:8032\n",
      "2024-11-15 19:05:52,152 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.29.0.3:8032\n",
      "2024-11-15 19:05:52,323 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1731686121000_0018\n",
      "2024-11-15 19:05:52,754 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-11-15 19:05:52,843 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-11-15 19:05:52,948 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1731686121000_0018\n",
      "2024-11-15 19:05:52,949 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-11-15 19:05:53,124 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-11-15 19:05:53,124 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-11-15 19:05:53,186 INFO impl.YarnClientImpl: Submitted application application_1731686121000_0018\n",
      "2024-11-15 19:05:53,212 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1731686121000_0018/\n",
      "2024-11-15 19:05:53,214 INFO mapreduce.Job: Running job: job_1731686121000_0018\n",
      "2024-11-15 19:05:59,346 INFO mapreduce.Job: Job job_1731686121000_0018 running in uber mode : false\n",
      "2024-11-15 19:05:59,348 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-11-15 19:06:04,410 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-11-15 19:06:08,435 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-11-15 19:06:08,452 INFO mapreduce.Job: Job job_1731686121000_0018 completed successfully\n",
      "2024-11-15 19:06:08,584 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=104\n",
      "\t\tFILE: Number of bytes written=945567\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2122\n",
      "\t\tHDFS: Number of bytes written=23\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5252\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1849\n",
      "\t\tTotal time spent by all map tasks (ms)=5252\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1849\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5252\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1849\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5378048\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1893376\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=50\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=80\n",
      "\t\tMap output materialized bytes=110\n",
      "\t\tInput split bytes=188\n",
      "\t\tCombine input records=16\n",
      "\t\tCombine output records=14\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce shuffle bytes=110\n",
      "\t\tReduce input records=14\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=28\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=159\n",
      "\t\tCPU time spent (ms)=2360\n",
      "\t\tPhysical memory (bytes) snapshot=956358656\n",
      "\t\tVirtual memory (bytes) snapshot=7807856640\n",
      "\t\tTotal committed heap usage (bytes)=770703360\n",
      "\t\tPeak Map Physical memory (bytes)=360284160\n",
      "\t\tPeak Map Virtual memory (bytes)=2600755200\n",
      "\t\tPeak Reduce Physical memory (bytes)=262369280\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2607251456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1934\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=23\n",
      "2024-11-15 19:06:08,585 INFO streaming.StreamJob: Output directory: /practica1/13_salida\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-files 11_mapper.py,11_combiner.py,13_reducer.py,countries.csv \\\n",
    "-mapper 11_mapper.py \\\n",
    "-combiner 11_combiner.py \\\n",
    "-reducer 13_reducer.py \\\n",
    "-input /practica1/ejercicio1/clients.csv \\\n",
    "-output /practica1/ejercicio1/13_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b7c73647-2075-4f48-846b-c198b60daee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t['Spain']\n",
      "3\t['Guam']\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -tail /practica1/ejercicio1/13_salida/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282d29b-7613-4a09-b5b5-c705ac947ba4",
   "metadata": {},
   "source": [
    "## Ejercicio 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202a812-00ed-4e67-8f25-68e8e79b3876",
   "metadata": {},
   "source": [
    "Se crearán tablas lo más específicas posible, de modo que se optimicen las consultas. Al hacer esto, podemos aprovechar al máximo el particionamiento eligiendo las columnas adecuadas para cada tabla. Luego, en consultas complejas podemos recurrir a joins de estas tablas. También podemos optimizar estos joins mediante el bucketizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "5b030306-94a9-473b-8968-0c3a15427ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -put /media/notebooks/practica1/convocatorias-2020.csv /practica1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b15d42-f520-4a15-ac1e-6cba7ee9c99a",
   "metadata": {},
   "source": [
    "Visualizamos los datos del archivo que vamos a usar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d4d4d5e9-5b9f-44ef-a56c-410cd9ca10c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40/51-0159-LPR19</td>\n",
       "      <td>381</td>\n",
       "      <td>381 - Estado Mayor General de La Fuerza Aérea</td>\n",
       "      <td>40</td>\n",
       "      <td>Departamento Contrataciones Córdoba - UOC 40/51</td>\n",
       "      <td>Licitacion Privada</td>\n",
       "      <td>Sin Modalidad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>02/01/2020 08:00:00 a.m.</td>\n",
       "      <td>10/01/2020 10:00:00 a.m.</td>\n",
       "      <td>Única</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>“Adquisición de Elementos de Librería para el ...</td>\n",
       "      <td>“Adquisición de Elementos de Librería para el ...</td>\n",
       "      <td>1957142.84</td>\n",
       "      <td>Proceso de Compra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22-0041-CDI19</td>\n",
       "      <td>250</td>\n",
       "      <td>250 - Caja de Retiros Jubilaciones y Pensiones...</td>\n",
       "      <td>22</td>\n",
       "      <td>22 - Dpto de Compras y Suministros - Caja de R...</td>\n",
       "      <td>Contratación Directa</td>\n",
       "      <td>Sin Modalidad</td>\n",
       "      <td>Apartado 1: Compulsa Abreviada Por Monto</td>\n",
       "      <td>2019</td>\n",
       "      <td>02/01/2020 12:00:00 p.m.</td>\n",
       "      <td>13/01/2020 09:00:00 a.m.</td>\n",
       "      <td>Única</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>Servicio de seguridad de información sensible ...</td>\n",
       "      <td>Servicio de seguridad de información sensible ...</td>\n",
       "      <td>119915.25</td>\n",
       "      <td>Proceso de Compra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0    1                                                  2   \\\n",
       "0  40/51-0159-LPR19  381      381 - Estado Mayor General de La Fuerza Aérea   \n",
       "1     22-0041-CDI19  250  250 - Caja de Retiros Jubilaciones y Pensiones...   \n",
       "\n",
       "   3                                                  4   \\\n",
       "0  40    Departamento Contrataciones Córdoba - UOC 40/51   \n",
       "1  22  22 - Dpto de Compras y Suministros - Caja de R...   \n",
       "\n",
       "                     5              6   \\\n",
       "0    Licitacion Privada  Sin Modalidad   \n",
       "1  Contratación Directa  Sin Modalidad   \n",
       "\n",
       "                                         7     8                         9   \\\n",
       "0                                       NaN  2019  02/01/2020 08:00:00 a.m.   \n",
       "1  Apartado 1: Compulsa Abreviada Por Monto  2019  02/01/2020 12:00:00 p.m.   \n",
       "\n",
       "                         10     11        12  \\\n",
       "0  10/01/2020 10:00:00 a.m.  Única  Nacional   \n",
       "1  13/01/2020 09:00:00 a.m.  Única  Nacional   \n",
       "\n",
       "                                                  13  \\\n",
       "0  “Adquisición de Elementos de Librería para el ...   \n",
       "1  Servicio de seguridad de información sensible ...   \n",
       "\n",
       "                                                  14          15  \\\n",
       "0  “Adquisición de Elementos de Librería para el ...  1957142.84   \n",
       "1  Servicio de seguridad de información sensible ...   119915.25   \n",
       "\n",
       "                  16  \n",
       "0  Proceso de Compra  \n",
       "1  Proceso de Compra  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"convocatorias-2020.csv\", header=None)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2625c05-40b1-4705-aca7-0575a7b0a666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>2</th>\n",
       "      <th>15</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Licitacion Privada</td>\n",
       "      <td>2019</td>\n",
       "      <td>381 - Estado Mayor General de La Fuerza Aérea</td>\n",
       "      <td>1957142.84</td>\n",
       "      <td>Nacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contratación Directa</td>\n",
       "      <td>2019</td>\n",
       "      <td>250 - Caja de Retiros Jubilaciones y Pensiones...</td>\n",
       "      <td>119915.25</td>\n",
       "      <td>Nacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contratación Directa</td>\n",
       "      <td>2019</td>\n",
       "      <td>381 - Estado Mayor General de La Fuerza Aérea</td>\n",
       "      <td>99000.00</td>\n",
       "      <td>Nacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contratación Directa</td>\n",
       "      <td>2019</td>\n",
       "      <td>116 - Biblioteca Nacional</td>\n",
       "      <td>1450050.00</td>\n",
       "      <td>Nacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contratación Directa</td>\n",
       "      <td>2020</td>\n",
       "      <td>604 - Dirección Nacional de Vialidad</td>\n",
       "      <td>684000.00</td>\n",
       "      <td>Nacional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     5     8   \\\n",
       "0    Licitacion Privada  2019   \n",
       "1  Contratación Directa  2019   \n",
       "2  Contratación Directa  2019   \n",
       "3  Contratación Directa  2019   \n",
       "4  Contratación Directa  2020   \n",
       "\n",
       "                                                  2           15        12  \n",
       "0      381 - Estado Mayor General de La Fuerza Aérea  1957142.84  Nacional  \n",
       "1  250 - Caja de Retiros Jubilaciones y Pensiones...   119915.25  Nacional  \n",
       "2      381 - Estado Mayor General de La Fuerza Aérea    99000.00  Nacional  \n",
       "3                          116 - Biblioteca Nacional  1450050.00  Nacional  \n",
       "4               604 - Dirección Nacional de Vialidad   684000.00  Nacional  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_select = df[[5, 8, 2, 15, 12]]\n",
    "df_select.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54b4386c-cc74-4f8c-aaee-ce68ad3b3d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo de Procedimiento: Hay un total de 6 valores posibles\n",
      "Ejercicio: Hay un total de 3 valores posibles\n",
      "Descripcion SAF: Hay un total de 125 valores posibles\n",
      "Alcance: Hay un total de 2 valores posibles\n"
     ]
    }
   ],
   "source": [
    "print(\"Tipo de Procedimiento: Hay un total de {} valores posibles\".format(df_select[5].nunique()))\n",
    "print(\"Ejercicio: Hay un total de {} valores posibles\".format(df_select[8].nunique()))\n",
    "print(\"Descripcion SAF: Hay un total de {} valores posibles\".format(df_select[2].nunique()))\n",
    "print(\"Alcance: Hay un total de {} valores posibles\".format(df_select[12].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1eed7-7725-450b-9835-cacfc8a90465",
   "metadata": {},
   "source": [
    "Voy a utilizar tablas internas ya que el particionado es más sencillo en estas. Para realizar particiones en tablas externas, se deben subir los\n",
    "archivos ya particionados adecuadamente a hdfs.\n",
    "\n",
    "Crearemos una tabla que temporal que contenga todos los datos del csv. De esa tabla extraeremos otras con columnas seleccionadas para llevar a cabo cada query de forma optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "a6ddbe8a-6b46-4be4-bcda-a10276680ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tabla_total.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile tabla_total.hql\n",
    "CREATE TABLE tabla_total(\n",
    "    Id STRING,\n",
    "    NroSaf INT,\n",
    "    DescripcionSaf STRING,\n",
    "    NroUoc INT,\n",
    "    Descripcion_Uoc STRING,\n",
    "    TipoProcedimiento STRING,\n",
    "    Modalidad STRING,\n",
    "    ApartadoDirecta STRING,\n",
    "    Ejercicio INT,\n",
    "    FechaPublicacion STRING,\n",
    "    FechaApertura STRING,\n",
    "    Etapa STRING,\n",
    "    Alcance STRING,\n",
    "    NombreProcedimiento STRING,\n",
    "    ObjetoProcedimiento STRING,\n",
    "    MontoEstimado FLOAT,\n",
    "    TipoOperacion STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/practica1/';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "e7ec96eb-2f61-403d-b212-88f097dcc79d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 296ed22e-a0bc-4127-993e-218fabfc1748\n",
      "24/11/20 20:58:43 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 20:58:44 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 20:58:44 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:58:44 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:58:44 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:58:44 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:58:44 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:58:44 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:58:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:58:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:58:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:58:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:58:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:58:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:58:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:58:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:58:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:58:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:58:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:58:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:58:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:58:46 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "0: jdbc:hive2://> CREATE TABLE tabla_total(\n",
      ". . . . . . . . >     Id STRING,\n",
      ". . . . . . . . >     NroSaf INT,\n",
      ". . . . . . . . >     DescripcionSaf STRING,\n",
      ". . . . . . . . >     NroUoc INT,\n",
      ". . . . . . . . >     Descripcion_Uoc STRING,\n",
      ". . . . . . . . >     TipoProcedimiento STRING,\n",
      ". . . . . . . . >     Modalidad STRING,\n",
      ". . . . . . . . >     ApartadoDirecta STRING,\n",
      ". . . . . . . . >     Ejercicio INT,\n",
      ". . . . . . . . >     FechaPublicacion STRING,\n",
      ". . . . . . . . >     FechaApertura STRING,\n",
      ". . . . . . . . >     Etapa STRING,\n",
      ". . . . . . . . >     Alcance STRING,\n",
      ". . . . . . . . >     NombreProcedimiento STRING,\n",
      ". . . . . . . . >     ObjetoProcedimiento STRING,\n",
      ". . . . . . . . >     MontoEstimado FLOAT,\n",
      ". . . . . . . . >     TipoOperacion STRING\n",
      ". . . . . . . . > )\n",
      ". . . . . . . . > ROW FORMAT DELIMITED\n",
      ". . . . . . . . > FIELDS TERMINATED BY ','\n",
      ". . . . . . . . > STORED AS TEXTFILE\n",
      ". . . . . . . . > LOCATION '/practica1/';\n",
      "No rows affected (1.8 seconds)\n",
      "0: jdbc:hive2://> \n",
      "0: jdbc:hive2://> Closing: 0: jdbc:hive2://\n",
      "24/11/20 20:58:48 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 20:58:48 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "# ejecucion del fichero hql mediante beeline\n",
    "!beeline -u \"jdbc:hive2://\" -f tabla_total.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "058283c0-abf9-4421-807d-0857ff0aa656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = af7abceb-673a-431c-806e-2c697b80ae1e\n",
      "24/11/20 21:13:05 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:13:05 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:13:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:08 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "Loading data to table default.tabla_total\n",
      "24/11/20 21:13:09 [HiveServer2-Background-Pool: Thread-61]: WARN metadata.Hive: Cannot get a table snapshot for tabla_total\n",
      "24/11/20 21:13:09 [HiveServer2-Background-Pool: Thread-61]: WARN metadata.Hive: Cannot get a table snapshot for tabla_total\n",
      "No rows affected (1.856 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 21:13:10 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 21:13:10 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "# carga datos 2021 \n",
    "!beeline -u \"jdbc:hive2://\" -e \\\n",
    "\"LOAD DATA INPATH '/practica1/convocatorias-2020.csv'  \\\n",
    "INTO TABLE tabla_total;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "31960474-937e-411f-bd74-d39e7a11ba14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 402ba315-b3a1-4c97-8104-4cb0ac9f705f\n",
      "24/11/20 21:13:16 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:13:17 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:13:17 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:17 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:17 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:17 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:17 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:17 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:18 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:18 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:18 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:18 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:18 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:18 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:19 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:19 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:19 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:19 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:19 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:19 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:19 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:19 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/20 21:13:22 [45b4ae00-6ed4-4d8a-a52c-bdc8f241126d main]: WARN calcite.RelOptHiveTable: No Stats for default@tabla_total, Columns: objetoprocedimiento, nrosaf, descripcion_uoc, etapa, descripcionsaf, fechaapertura, tipoprocedimiento, ejercicio, alcance, nrouoc, apartadodirecta, montoestimado, tipooperacion, fechapublicacion, modalidad, nombreprocedimiento, id\n",
      "No Stats for default@tabla_total, Columns: objetoprocedimiento, nrosaf, descripcion_uoc, etapa, descripcionsaf, fechaapertura, tipoprocedimiento, ejercicio, alcance, nrouoc, apartadodirecta, montoestimado, tipooperacion, fechapublicacion, modalidad, nombreprocedimiento, id\n",
      "24/11/20 21:13:22 [45b4ae00-6ed4-4d8a-a52c-bdc8f241126d main]: WARN optimizer.SimpleFetchOptimizer: Table default@tabla_total is external table, falling back to filesystem scan.\n",
      "+-------------------+---------------------+----------------------------------------------------+---------------------+----------------------------------------------------+--------------------------------+------------------------+-------------------------------------------+------------------------+-------------------------------+----------------------------+--------------------+----------------------+----------------------------------------------------+----------------------------------------------------+----------------------------+----------------------------+\n",
      "|  tabla_total.id   | tabla_total.nrosaf  |             tabla_total.descripcionsaf             | tabla_total.nrouoc  |            tabla_total.descripcion_uoc             | tabla_total.tipoprocedimiento  | tabla_total.modalidad  |        tabla_total.apartadodirecta        | tabla_total.ejercicio  | tabla_total.fechapublicacion  | tabla_total.fechaapertura  | tabla_total.etapa  | tabla_total.alcance  |          tabla_total.nombreprocedimiento           |          tabla_total.objetoprocedimiento           | tabla_total.montoestimado  | tabla_total.tipooperacion  |\n",
      "+-------------------+---------------------+----------------------------------------------------+---------------------+----------------------------------------------------+--------------------------------+------------------------+-------------------------------------------+------------------------+-------------------------------+----------------------------+--------------------+----------------------+----------------------------------------------------+----------------------------------------------------+----------------------------+----------------------------+\n",
      "| 40/51-0159-LPR19  | 381                 | 381 - Estado Mayor General de La Fuerza Aérea      | 40                  | Departamento Contrataciones Córdoba - UOC 40/51    | Licitacion Privada             | Sin Modalidad          |                                           | 2019                   | 02/01/2020 08:00:00 a.m.      | 10/01/2020 10:00:00 a.m.   | Única              | Nacional             | “Adquisición de Elementos de Librería para el HOSPITAL AERONÁUTICO CÓRDOBA | “Adquisición de Elementos de Librería para el HOSPITAL AERONÁUTICO CÓRDOBA | 1957142.9                  | Proceso de Compra          |\n",
      "| 22-0041-CDI19     | 250                 | 250 - Caja de Retiros Jubilaciones y Pensiones de la Policía Federal | 22                  | 22 - Dpto de Compras y Suministros - Caja de Retiros Jubilaciones y Pensiones de la Policía Federal | Contratación Directa           | Sin Modalidad          | Apartado 1: Compulsa Abreviada Por Monto  | 2019                   | 02/01/2020 12:00:00 p.m.      | 13/01/2020 09:00:00 a.m.   | Única              | Nacional             | Servicio de seguridad de información sensible de la página web del Organismo. | Servicio de seguridad de información sensible de la página web del Organismo. | 119915.25                  | Proceso de Compra          |\n",
      "| 40/51-0918-CDI19  | 381                 | 381 - Estado Mayor General de La Fuerza Aérea      | 40                  | Departamento Contrataciones Córdoba - UOC 40/51    | Contratación Directa           | Sin Modalidad          | Apartado 1: Compulsa Abreviada Por Monto  | 2019                   | 02/01/2020 12:00:00 p.m.      | 13/01/2020 10:00:00 a.m.   | Única              | Nacional             | “Servicio Médico Asistencial para Organismos Varios de la GACBA . | “Servicio Médico Asistencial para Organismos Varios de la GACBA . | 99000.0                    | Proceso de Compra          |\n",
      "+-------------------+---------------------+----------------------------------------------------+---------------------+----------------------------------------------------+--------------------------------+------------------------+-------------------------------------------+------------------------+-------------------------------+----------------------------+--------------------+----------------------+----------------------------------------------------+----------------------------------------------------+----------------------------+----------------------------+\n",
      "3 rows selected (3.304 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 21:13:23 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 21:13:23 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \\\n",
    "\"SELECT * FROM tabla_total \\\n",
    "LIMIT 3;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deabda09-69d4-415f-ad33-d65761cff5fd",
   "metadata": {},
   "source": [
    "Creamos el resto de las tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "af774b4b-d90a-4005-a161-1bc8659c11c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tabla_procedimiento_ejercicio.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile tabla_procedimiento_ejercicio.hql\n",
    "CREATE TABLE IF NOT EXISTS tabla_procedimiento_ejercicio(\n",
    "    ID STRING\n",
    ")\n",
    "PARTITIONED BY (Ejercicio Int, Tipo_Procedimiento STRING)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/practica1/';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5ae0b-876b-4c2f-bad4-80dcc33c3ce2",
   "metadata": {},
   "source": [
    "La bucketización ayuda a reducir el número de valores únicos por bucket, lo que acelera el procesado de la operación de conteo de valores únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "1e681008-425d-4d94-958b-0ba2bcb77df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tabla_saf.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile tabla_saf.hql\n",
    "CREATE TABLE IF NOT EXISTS tabla_saf(\n",
    "    ID STRING,\n",
    "    Descripcion_SAF STRING\n",
    ")\n",
    "CLUSTERED BY (Descripcion_SAF) INTO 5 BUCKETS;\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/practica1/';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea8e6a4-cde9-4b95-8753-7663d00f62c0",
   "metadata": {},
   "source": [
    "Bucketizar los datos por una columna que esté relacionada con el cálculo puede acelerar las consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ab65b351-4146-4d6f-8072-ddffb7866ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tabla_monto_alcance.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile tabla_monto_alcance.hql\n",
    "CREATE TABLE IF NOT EXISTS tabla_monto_alcance(\n",
    "    ID STRING,\n",
    "    Monto_Estimado FLOAT\n",
    ")\n",
    "PARTITIONED BY (alcance STRING)\n",
    "CLUSTERED BY (Monto_Estimado) INTO 10 BUCKETS;\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/practica1/';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "869a8972-b9dd-478b-952a-44861ce4c268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Si queremos resetear las tablas antes de crearlas\n",
    "\n",
    "#!beeline -u \"jdbc:hive2://\" -e \"drop table tabla_procedimiento_ejercicio;\"\n",
    "# !beeline -u \"jdbc:hive2://\" -e \"drop table alcance;\"\n",
    "# !beeline -u \"jdbc:hive2://\" -e \"drop table ejercicio;\"\n",
    "# !beeline -u \"jdbc:hive2://\" -e \"drop table monto;\"\n",
    "# !beeline -u \"jdbc:hive2://\" -e \"drop table saf;\"\n",
    "# !beeline -u \"jdbc:hive2://\" -e \"drop table stars;\"\n",
    "#!beeline -u \"jdbc:hive2://\" -e \"drop table tabla_total;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d6af8837-502a-4e7a-893e-23594d26ee1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = b2f67ba7-a67d-48a5-9d67-7157b16aeed4\n",
      "24/11/20 18:34:13 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 18:34:14 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 18:34:14 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:14 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:14 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:14 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:14 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:14 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:15 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:15 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:15 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:15 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:15 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:15 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:16 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:16 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:16 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:16 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:16 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:16 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:16 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:16 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "0: jdbc:hive2://> CREATE TABLE IF NOT EXISTS tabla_procedimiento_ejercicio(\n",
      ". . . . . . . . >     ID STRING\n",
      ". . . . . . . . > )\n",
      ". . . . . . . . > PARTITIONED BY (Ejercicio DATE, Tipo_Procedimiento STRING)\n",
      ". . . . . . . . > ROW FORMAT DELIMITED\n",
      ". . . . . . . . > FIELDS TERMINATED BY ','\n",
      ". . . . . . . . > STORED AS TEXTFILE\n",
      ". . . . . . . . > LOCATION '/practica1/';\n",
      "No rows affected (1.533 seconds)\n",
      "0: jdbc:hive2://> \n",
      "0: jdbc:hive2://> Closing: 0: jdbc:hive2://\n",
      "24/11/20 18:34:18 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 18:34:18 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 1346b0d0-2e9e-4ab6-bb51-cdf69ee9b052\n",
      "24/11/20 18:34:22 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 18:34:23 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 18:34:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:25 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "0: jdbc:hive2://> CREATE TABLE IF NOT EXISTS tabla_saf(\n",
      ". . . . . . . . >     ID STRING,\n",
      ". . . . . . . . >     Descripcion_SAF STRING\n",
      ". . . . . . . . > )\n",
      ". . . . . . . . > CLUSTERED BY (Descripcion_SAF) INTO 5 BUCKETS;\n",
      "No rows affected (1.537 seconds)\n",
      "0: jdbc:hive2://> ROW FORMAT DELIMITED\n",
      ". . . . . . . . > FIELDS TERMINATED BY ','\n",
      ". . . . . . . . > STORED AS TEXTFILE\n",
      ". . . . . . . . > LOCATION '/practica1/';\n",
      "FAILED: ParseException line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED'\n",
      "24/11/20 18:34:27 [2711c10a-793f-4891-a2cf-c00a820356e3 main]: ERROR ql.Driver: FAILED: ParseException line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED'\n",
      "org.apache.hadoop.hive.ql.parse.ParseException: line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED'\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:125)\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:97)\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:89)\n",
      "\tat org.apache.hadoop.hive.ql.Compiler.parse(Compiler.java:172)\n",
      "\tat org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:105)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:519)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:471)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:436)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:430)\n",
      "\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:121)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:207)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:270)\n",
      "\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:286)\n",
      "\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:557)\n",
      "\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:542)\n",
      "\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:316)\n",
      "\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:652)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hive.jdbc.HiveConnection$SynchronizedHandler.invoke(HiveConnection.java:2224)\n",
      "\tat com.sun.proxy.$Proxy46.ExecuteStatement(Unknown Source)\n",
      "\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:353)\n",
      "\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:293)\n",
      "\tat org.apache.hive.beeline.Commands.executeInternal(Commands.java:1007)\n",
      "\tat org.apache.hive.beeline.Commands.execute(Commands.java:1216)\n",
      "\tat org.apache.hive.beeline.Commands.sql(Commands.java:1145)\n",
      "\tat org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:1519)\n",
      "\tat org.apache.hive.beeline.BeeLine.execute(BeeLine.java:1379)\n",
      "\tat org.apache.hive.beeline.BeeLine.executeFile(BeeLine.java:1353)\n",
      "\tat org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1142)\n",
      "\tat org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1097)\n",
      "\tat org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:555)\n",
      "\tat org.apache.hive.beeline.BeeLine.main(BeeLine.java:537)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:330)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:245)\n",
      "Caused by: NoViableAltException(307@[])\n",
      "\tat org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1610)\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:123)\n",
      "\t... 40 more\n",
      "\n",
      "24/11/20 18:34:27 [main]: ERROR thrift.ThriftCLIService: Failed to execute statement [request: TExecuteStatementReq(sessionHandle:TSessionHandle(sessionId:THandleIdentifier(guid:27 11 C1 0A 79 3F 48 91 A2 CF C0 0A 82 03 56 E3, secret:95 86 A4 A3 AA 91 4A 27 85 11 80 50 B6 38 42 9B)), statement:ROW FORMAT DELIMITED\n",
      "FIELDS TERMINATED BY ','\n",
      "STORED AS TEXTFILE\n",
      "LOCATION '/practica1/', confOverlay:{}, runAsync:true, queryTimeout:0)]\n",
      "org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED'\n",
      "\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:376) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:214) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:270) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:286) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:557) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:542) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:316) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:652) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_412]\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_412]\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_412]\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_412]\n",
      "\tat org.apache.hive.jdbc.HiveConnection$SynchronizedHandler.invoke(HiveConnection.java:2224) ~[hive-jdbc-4.0.0.jar:4.0.0]\n",
      "\tat com.sun.proxy.$Proxy46.ExecuteStatement(Unknown Source) ~[?:?]\n",
      "\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:353) ~[hive-jdbc-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:293) ~[hive-jdbc-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.Commands.executeInternal(Commands.java:1007) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.Commands.execute(Commands.java:1216) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.Commands.sql(Commands.java:1145) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:1519) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.execute(BeeLine.java:1379) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.executeFile(BeeLine.java:1353) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1142) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1097) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:555) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.main(BeeLine.java:537) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_412]\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_412]\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_412]\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_412]\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:330) ~[hadoop-common-3.4.0.jar:?]\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:245) ~[hadoop-common-3.4.0.jar:?]\n",
      "Caused by: org.apache.hadoop.hive.ql.parse.ParseException: line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED'\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:125) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:97) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:89) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Compiler.parse(Compiler.java:172) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:105) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:519) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:471) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:436) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:430) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:121) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:207) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\t... 30 more\n",
      "Caused by: org.antlr.runtime.NoViableAltException\n",
      "\tat org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1610) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:123) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:97) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:89) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Compiler.parse(Compiler.java:172) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:105) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:519) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:471) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:436) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:430) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:121) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:207) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\t... 30 more\n",
      "Error: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED' (state=42000,code=40000)\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 18:34:27 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 18:34:27 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 3821c299-57e1-49a6-a6a8-d81b9719b256\n",
      "24/11/20 18:34:31 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 18:34:31 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 18:34:32 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:32 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:32 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:32 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:32 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:32 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:33 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:33 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:33 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:33 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:33 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:33 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 18:34:34 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:34 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:34 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:34 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:34 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:34 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:34 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 18:34:34 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "0: jdbc:hive2://> CREATE TABLE IF NOT EXISTS tabla_monto_alcance(\n",
      ". . . . . . . . >     ID STRING,\n",
      ". . . . . . . . >     Monto_Estimado FLOAT\n",
      ". . . . . . . . > )\n",
      ". . . . . . . . > PARTITIONED BY (alcance STRING)\n",
      ". . . . . . . . > CLUSTERED BY (Monto_Estimado) INTO 10 BUCKETS;\n",
      "No rows affected (1.668 seconds)\n",
      "0: jdbc:hive2://> ROW FORMAT DELIMITED\n",
      ". . . . . . . . > FIELDS TERMINATED BY ','\n",
      ". . . . . . . . > STORED AS TEXTFILE\n",
      ". . . . . . . . > LOCATION '/practica1/';\n",
      "FAILED: ParseException line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED'\n",
      "24/11/20 18:34:36 [eff94ea0-1318-41d6-9d66-0a9f5a0ff371 main]: ERROR ql.Driver: FAILED: ParseException line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED'\n",
      "org.apache.hadoop.hive.ql.parse.ParseException: line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED'\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:125)\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:97)\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:89)\n",
      "\tat org.apache.hadoop.hive.ql.Compiler.parse(Compiler.java:172)\n",
      "\tat org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:105)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:519)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:471)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:436)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:430)\n",
      "\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:121)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:207)\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:270)\n",
      "\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:286)\n",
      "\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:557)\n",
      "\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:542)\n",
      "\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:316)\n",
      "\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:652)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hive.jdbc.HiveConnection$SynchronizedHandler.invoke(HiveConnection.java:2224)\n",
      "\tat com.sun.proxy.$Proxy46.ExecuteStatement(Unknown Source)\n",
      "\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:353)\n",
      "\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:293)\n",
      "\tat org.apache.hive.beeline.Commands.executeInternal(Commands.java:1007)\n",
      "\tat org.apache.hive.beeline.Commands.execute(Commands.java:1216)\n",
      "\tat org.apache.hive.beeline.Commands.sql(Commands.java:1145)\n",
      "\tat org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:1519)\n",
      "\tat org.apache.hive.beeline.BeeLine.execute(BeeLine.java:1379)\n",
      "\tat org.apache.hive.beeline.BeeLine.executeFile(BeeLine.java:1353)\n",
      "\tat org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1142)\n",
      "\tat org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1097)\n",
      "\tat org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:555)\n",
      "\tat org.apache.hive.beeline.BeeLine.main(BeeLine.java:537)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:330)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:245)\n",
      "Caused by: NoViableAltException(307@[])\n",
      "\tat org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1610)\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:123)\n",
      "\t... 40 more\n",
      "\n",
      "24/11/20 18:34:36 [main]: ERROR thrift.ThriftCLIService: Failed to execute statement [request: TExecuteStatementReq(sessionHandle:TSessionHandle(sessionId:THandleIdentifier(guid:EF F9 4E A0 13 18 41 D6 9D 66 0A 9F 5A 0F F3 71, secret:52 0E 40 3E 2A 10 4D B3 89 8F F8 47 AC CB B6 DA)), statement:ROW FORMAT DELIMITED\n",
      "FIELDS TERMINATED BY ','\n",
      "STORED AS TEXTFILE\n",
      "LOCATION '/practica1/', confOverlay:{}, runAsync:true, queryTimeout:0)]\n",
      "org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED'\n",
      "\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:376) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:214) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:270) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:286) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:557) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:542) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:316) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:652) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_412]\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_412]\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_412]\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_412]\n",
      "\tat org.apache.hive.jdbc.HiveConnection$SynchronizedHandler.invoke(HiveConnection.java:2224) ~[hive-jdbc-4.0.0.jar:4.0.0]\n",
      "\tat com.sun.proxy.$Proxy46.ExecuteStatement(Unknown Source) ~[?:?]\n",
      "\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:353) ~[hive-jdbc-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:293) ~[hive-jdbc-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.Commands.executeInternal(Commands.java:1007) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.Commands.execute(Commands.java:1216) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.Commands.sql(Commands.java:1145) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:1519) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.execute(BeeLine.java:1379) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.executeFile(BeeLine.java:1353) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1142) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1097) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:555) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.beeline.BeeLine.main(BeeLine.java:537) ~[hive-beeline-4.0.0.jar:4.0.0]\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_412]\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_412]\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_412]\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_412]\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:330) ~[hadoop-common-3.4.0.jar:?]\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:245) ~[hadoop-common-3.4.0.jar:?]\n",
      "Caused by: org.apache.hadoop.hive.ql.parse.ParseException: line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED'\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:125) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:97) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:89) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Compiler.parse(Compiler.java:172) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:105) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:519) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:471) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:436) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:430) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:121) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:207) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\t... 30 more\n",
      "Caused by: org.antlr.runtime.NoViableAltException\n",
      "\tat org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1610) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:123) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:97) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:89) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Compiler.parse(Compiler.java:172) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:105) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:519) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:471) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:436) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:430) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:121) ~[hive-exec-4.0.0.jar:4.0.0]\n",
      "\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:207) ~[hive-service-4.0.0.jar:4.0.0]\n",
      "\t... 30 more\n",
      "Error: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED' (state=42000,code=40000)\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 18:34:36 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 18:34:36 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "# ejecucion del fichero hql mediante beeline\n",
    "!beeline -u \"jdbc:hive2://\" -f tabla_procedimiento_ejercicio.hql\n",
    "!beeline -u \"jdbc:hive2://\" -f tabla_saf.hql\n",
    "!beeline -u \"jdbc:hive2://\" -f tabla_monto_alcance.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "73f0677f-26a3-46f5-9c07-981377f129aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ejecucion del fichero hql mediante beeline\n",
    "# !beeline -u \"jdbc:hive2://\" -f tabla_procedimiento.hql\n",
    "# !beeline -u \"jdbc:hive2://\" -f tabla_procedimiento.hql\n",
    "# !beeline -u \"jdbc:hive2://\" -f tabla_ejercicio.hql\n",
    "# !beeline -u \"jdbc:hive2://\" -f tabla_saf.hql\n",
    "# !beeline -u \"jdbc:hive2://\" -f tabla_monto.hql\n",
    "# !beeline -u \"jdbc:hive2://\" -f tabla_alcance.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "5d8a2510-24f5-4ccd-8ea4-0fb088b76b20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! beeline -u \"jdbc:hive2://\" -e \"show tables;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa7387-0df2-4701-98ad-88c4c58b0926",
   "metadata": {},
   "source": [
    "**Poblamos las tablas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "50c689cd-5ac5-4db0-8a46-8221df691bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 933e8eb8-d822-4514-a4d3-05eb21a84a4c\n",
      "24/11/20 21:13:47 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:13:48 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:13:48 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:48 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:48 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:48 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:48 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:48 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:49 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:49 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:49 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:49 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:49 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:49 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:13:50 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:50 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:50 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:50 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:50 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:50 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:50 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:13:50 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/20 21:13:53 [7708df13-69cd-43fa-98bc-66899594e85d main]: WARN calcite.RelOptHiveTable: No Stats for default@tabla_total, Columns: id, tipoprocedimiento, ejercicio\n",
      "No Stats for default@tabla_total, Columns: id, tipoprocedimiento, ejercicio\n",
      "24/11/20 21:13:53 [HiveServer2-Background-Pool: Thread-61]: WARN ql.Driver: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez) or using Hive 1.X releases.\n",
      "Query ID = root_20241120211350_d15a7d9a-28ee-4a64-bf73-d80c4d1f52f9\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "24/11/20 21:13:54 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "WARN  : Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez) or using Hive 1.X releases.\n",
      "Starting Job = job_1732132153600_0010, Tracking URL = http://yarnmanager:8088/proxy/application_1732132153600_0010/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1732132153600_0010\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "24/11/20 21:14:00 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead\n",
      "2024-11-20 21:14:00,576 Stage-1 map = 0%,  reduce = 0%\n",
      "2024-11-20 21:14:07,762 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.08 sec\n",
      "2024-11-20 21:14:12,921 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.14 sec\n",
      "MapReduce Total cumulative CPU time: 9 seconds 140 msec\n",
      "Ended Job = job_1732132153600_0010\n",
      "Loading data to table default.tabla_procedimiento_ejercicio partition (ejercicio=null, tipo_procedimiento=null)\n",
      "\n",
      "\n",
      "\t Time taken to load dynamic partitions: 0.344 seconds\n",
      "\t Time taken for adding to write entity : 0.002 seconds\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "24/11/20 21:14:14 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "Starting Job = job_1732132153600_0011, Tracking URL = http://yarnmanager:8088/proxy/application_1732132153600_0011/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1732132153600_0011\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1\n",
      "24/11/20 21:14:25 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead\n",
      "2024-11-20 21:14:25,095 Stage-3 map = 0%,  reduce = 0%\n",
      "2024-11-20 21:14:29,188 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 4.08 sec\n",
      "2024-11-20 21:14:36,393 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 7.69 sec\n",
      "MapReduce Total cumulative CPU time: 7 seconds 690 msec\n",
      "Ended Job = job_1732132153600_0011\n",
      "24/11/20 21:14:37 [HiveServer2-Background-Pool: Thread-61]: WARN metastore.ObjectStore: Falling back to ORM path due to direct SQL failure (this is not an error): java.lang.ClassCastException: org.apache.derby.impl.jdbc.EmbedClob cannot be cast to java.lang.String at org.apache.hadoop.hive.metastore.ExceptionHandler.newMetaException(ExceptionHandler.java:152) at org.apache.hadoop.hive.metastore.Batchable.runBatched(Batchable.java:92) at org.apache.hadoop.hive.metastore.DirectSqlUpdatePart.getParams(DirectSqlUpdatePart.java:753) at org.apache.hadoop.hive.metastore.DirectSqlUpdatePart.updateParamTableInBatch(DirectSqlUpdatePart.java:720) at org.apache.hadoop.hive.metastore.DirectSqlUpdatePart.alterPartitions(DirectSqlUpdatePart.java:641) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.alterPartitions(MetaStoreDirectSql.java:598) at org.apache.hadoop.hive.metastore.ObjectStore$18.getSqlResult(ObjectStore.java:5300);\n",
      " Caused by: org.apache.derby.impl.jdbc.EmbedClob cannot be cast to java.lang.String at org.apache.hadoop.hive.metastore.DirectSqlUpdatePart$3.run(DirectSqlUpdatePart.java:764) at org.apache.hadoop.hive.metastore.Batchable.runBatched(Batchable.java:79)\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 9.14 sec   HDFS Read: 5702369 HDFS Write: 222167 HDFS EC Read: 0 SUCCESS\n",
      "Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 7.69 sec   HDFS Read: 20010 HDFS Write: 8827 HDFS EC Read: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 16 seconds 830 msec\n",
      "13,549 rows affected (47.505 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 21:14:38 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 21:14:38 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://\" -e \\\n",
    "\"INSERT INTO TABLE tabla_procedimiento_ejercicio (ID, Ejercicio, Tipo_Procedimiento) \\\n",
    "SELECT Id, Ejercicio, TipoProcedimiento FROM tabla_total;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "80af0a10-7225-4bde-8047-e704e5a84522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 7a8af492-658d-46fe-a66e-3263f05ec6ac\n",
      "24/11/20 21:15:22 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:15:22 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:15:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:15:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:15:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:15:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:15:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:15:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:15:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:15:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:15:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:15:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:15:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:15:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:15:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:15:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:15:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:15:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:15:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:15:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:15:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:15:24 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/20 21:15:27 [d740f67d-2d4d-4412-8c6a-005a23b25eec main]: WARN DataNucleus.Datastore: SQL Warning : Null values were eliminated from the argument of a column function.\n",
      "24/11/20 21:15:27 [d740f67d-2d4d-4412-8c6a-005a23b25eec main]: WARN optimizer.SimpleFetchOptimizer: Table default@tabla_procedimiento_ejercicio is external table, falling back to filesystem scan.\n",
      "+-----------------------------------+------------------------------------------+---------------------------------------------------+\n",
      "| tabla_procedimiento_ejercicio.id  | tabla_procedimiento_ejercicio.ejercicio  | tabla_procedimiento_ejercicio.tipo_procedimiento  |\n",
      "+-----------------------------------+------------------------------------------+---------------------------------------------------+\n",
      "| 31-0003-CDI17                     | 2017                                     | Contratación Directa                              |\n",
      "| 31-0019-LPU17                     | 2017                                     | Licitacion Pública                                |\n",
      "| 16-0021-CPR19                     | 2019                                     | Concurso Privado                                  |\n",
      "+-----------------------------------+------------------------------------------+---------------------------------------------------+\n",
      "3 rows selected (3.284 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 21:15:28 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 21:15:28 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \"SELECT * FROM tabla_procedimiento_ejercicio \\\n",
    "LIMIT 3;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "ccd82b62-97f8-45dc-a309-ea014165c896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 6b8aada2-1e43-46bb-8cd9-dff02818b506\n",
      "24/11/20 20:21:05 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 20:21:06 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 20:21:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:21:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:21:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:21:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:21:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:21:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:21:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:21:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:21:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:21:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:21:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:21:06 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:21:08 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:21:08 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:21:08 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:21:08 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:21:08 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:21:08 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:21:08 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:21:08 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/20 20:21:11 [633b691e-3886-48bd-ae6b-adaf137f98ad main]: WARN calcite.RelOptHiveTable: No Stats for default@tabla_total, Columns: descripcionsaf, id\n",
      "No Stats for default@tabla_total, Columns: descripcionsaf, id\n",
      "24/11/20 20:21:11 [HiveServer2-Background-Pool: Thread-61]: WARN ql.Driver: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez) or using Hive 1.X releases.\n",
      "Query ID = root_20241120202108_b9d19186-4500-4ece-8956-abf9a69a4662\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks determined at compile time: 5\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "24/11/20 20:21:12 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "WARN  : Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez) or using Hive 1.X releases.\n",
      "Starting Job = job_1732132153600_0004, Tracking URL = http://yarnmanager:8088/proxy/application_1732132153600_0004/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1732132153600_0004\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 5\n",
      "24/11/20 20:21:18 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead\n",
      "2024-11-20 20:21:18,671 Stage-1 map = 0%,  reduce = 0%\n",
      "2024-11-20 20:21:26,949 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.27 sec\n",
      "2024-11-20 20:22:04,970 Stage-1 map = 100%,  reduce = 40%, Cumulative CPU 66.73 sec\n",
      "2024-11-20 20:22:06,008 Stage-1 map = 100%,  reduce = 53%, Cumulative CPU 71.77 sec\n",
      "2024-11-20 20:22:07,036 Stage-1 map = 100%,  reduce = 60%, Cumulative CPU 76.39 sec\n",
      "2024-11-20 20:22:09,090 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 86.13 sec\n",
      "2024-11-20 20:23:09,858 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 86.13 sec\n",
      "2024-11-20 20:24:10,471 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 86.13 sec\n",
      "MapReduce Total cumulative CPU time: 1 minutes 26 seconds 130 msec\n",
      "Ended Job = job_1732132153600_0004\n",
      "Loading data to table default.tabla_saf\n",
      "24/11/20 20:25:10 [HiveServer2-Background-Pool: Thread-61]: WARN metadata.Hive: Cannot get a table snapshot for tabla_saf\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "24/11/20 20:25:10 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "Starting Job = job_1732132153600_0005, Tracking URL = http://yarnmanager:8088/proxy/application_1732132153600_0005/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1732132153600_0005\n",
      "Interrupting... Please be patient this may take some time.\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://\" -e \\\n",
    "\"INSERT INTO TABLE tabla_saf (ID, Descripcion_SAF) \\\n",
    "SELECT Id, DescripcionSaf FROM tabla_total;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "95464a4e-625a-4dcb-b5af-ba3693ef2dbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = d12822f2-7514-47a1-9b67-316f19b12a84\n",
      "24/11/20 20:28:20 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 20:28:22 [main]: ERROR pool.HikariPool: objectstore - Exception during pool initialization.\n",
      "java.sql.SQLException: Failed to start database '/usr/local/hive/metastore_db' with class loader sun.misc.Launcher$AppClassLoader@26f67b76, see the next exception for details.\n",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_412]\n",
      "\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.jdbc.InternalDriver$LoginCallable.call(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.jdbc.InternalDriver$LoginCallable.call(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_412]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_412]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_412]\n",
      "\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_412]\n",
      "Caused by: org.apache.derby.iapi.error.StandardException: Failed to start database '/usr/local/hive/metastore_db' with class loader sun.misc.Launcher$AppClassLoader@26f67b76, see the next exception for details.\n",
      "\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\t... 15 more\n",
      "Caused by: org.apache.derby.iapi.error.StandardException: Another instance of Derby may have already booted the database /usr/local/hive/metastore_db.\n",
      "\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_412]\n",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_412]\n",
      "\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_412]\n",
      "\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_412]\n",
      "\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\tat java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_412]\n",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source) ~[derby-10.14.2.0.jar:?]\n",
      "\t... 12 more\n",
      "24/11/20 20:28:22 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 20:28:22 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 20:28:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:28:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:28:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:28:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:28:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:28:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:28:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:28:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:28:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:28:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:28:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:28:23 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:28:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:28:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:28:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:28:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:28:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:28:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:28:24 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:28:24 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/20 20:28:28 [175f972b-b080-4ae5-9185-cdc8db1adf74 main]: WARN optimizer.SimpleFetchOptimizer: Cannot determine basic stats for table: default@tabla_saf from metastore. Falling back.\n",
      "+----------------+----------------------------------------------------+\n",
      "|  tabla_saf.id  |             tabla_saf.descripcion_saf              |\n",
      "+----------------+----------------------------------------------------+\n",
      "| 91-0009-LPU19  | 103 - Consejo Nacional de Investigaciones Científicas y Técnicas |\n",
      "| 91-0003-LPR19  | 103 - Consejo Nacional de Investigaciones Científicas y Técnicas |\n",
      "| 91-0015-CDI20  | 103 - Consejo Nacional de Investigaciones Científicas y Técnicas |\n",
      "+----------------+----------------------------------------------------+\n",
      "3 rows selected (3.417 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 20:28:28 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 20:28:28 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \"SELECT * FROM tabla_saf \\\n",
    "LIMIT 3;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "1c3ebcf8-4ca9-465d-aae1-716edee28d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = f99bd234-149f-45cd-872d-22f2d8484745\n",
      "24/11/20 20:12:23 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 20:12:23 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 20:12:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:12:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:12:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:12:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:12:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:12:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:12:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:12:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:12:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:12:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:12:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:12:24 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:12:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:12:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:12:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:12:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:12:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:12:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:12:25 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:12:25 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/20 20:12:28 [a8515e5b-01bf-4ce8-916d-18bc5d9217e3 main]: WARN calcite.RelOptHiveTable: No Stats for default@tabla_total, Columns: montoestimado, id, alcance\n",
      "No Stats for default@tabla_total, Columns: montoestimado, id, alcance\n",
      "24/11/20 20:12:28 [HiveServer2-Background-Pool: Thread-61]: WARN ql.Driver: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez) or using Hive 1.X releases.\n",
      "Query ID = root_20241120201225_e9257807-5d2d-4dfb-a540-e4d378ec3ec5\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "24/11/20 20:12:29 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "WARN  : Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez) or using Hive 1.X releases.\n",
      "Starting Job = job_1732132153600_0002, Tracking URL = http://yarnmanager:8088/proxy/application_1732132153600_0002/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1732132153600_0002\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "24/11/20 20:12:36 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead\n",
      "2024-11-20 20:12:36,179 Stage-1 map = 0%,  reduce = 0%\n",
      "2024-11-20 20:12:42,401 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.71 sec\n",
      "2024-11-20 20:12:47,534 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.96 sec\n",
      "MapReduce Total cumulative CPU time: 11 seconds 960 msec\n",
      "Ended Job = job_1732132153600_0002\n",
      "Loading data to table default.tabla_monto_alcance partition (alcance=null)\n",
      "\n",
      "\n",
      "\t Time taken to load dynamic partitions: 0.258 seconds\n",
      "\t Time taken for adding to write entity : 0.002 seconds\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "24/11/20 20:12:49 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "Starting Job = job_1732132153600_0003, Tracking URL = http://yarnmanager:8088/proxy/application_1732132153600_0003/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1732132153600_0003\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1\n",
      "24/11/20 20:12:59 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead\n",
      "2024-11-20 20:12:59,807 Stage-3 map = 0%,  reduce = 0%\n",
      "2024-11-20 20:13:05,990 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 4.25 sec\n",
      "2024-11-20 20:13:11,124 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 8.12 sec\n",
      "MapReduce Total cumulative CPU time: 8 seconds 120 msec\n",
      "Ended Job = job_1732132153600_0003\n",
      "24/11/20 20:13:13 [HiveServer2-Background-Pool: Thread-61]: WARN metastore.ObjectStore: Falling back to ORM path due to direct SQL failure (this is not an error): java.lang.ClassCastException: org.apache.derby.impl.jdbc.EmbedClob cannot be cast to java.lang.String at org.apache.hadoop.hive.metastore.ExceptionHandler.newMetaException(ExceptionHandler.java:152) at org.apache.hadoop.hive.metastore.Batchable.runBatched(Batchable.java:92) at org.apache.hadoop.hive.metastore.DirectSqlUpdatePart.getParams(DirectSqlUpdatePart.java:753) at org.apache.hadoop.hive.metastore.DirectSqlUpdatePart.updateParamTableInBatch(DirectSqlUpdatePart.java:720) at org.apache.hadoop.hive.metastore.DirectSqlUpdatePart.alterPartitions(DirectSqlUpdatePart.java:641) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.alterPartitions(MetaStoreDirectSql.java:598) at org.apache.hadoop.hive.metastore.ObjectStore$18.getSqlResult(ObjectStore.java:5300);\n",
      " Caused by: org.apache.derby.impl.jdbc.EmbedClob cannot be cast to java.lang.String at org.apache.hadoop.hive.metastore.DirectSqlUpdatePart$3.run(DirectSqlUpdatePart.java:764) at org.apache.hadoop.hive.metastore.Batchable.runBatched(Batchable.java:79)\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 11.96 sec   HDFS Read: 5705910 HDFS Write: 345198 HDFS EC Read: 0 SUCCESS\n",
      "Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 8.12 sec   HDFS Read: 19886 HDFS Write: 3967 HDFS EC Read: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 20 seconds 80 msec\n",
      "13,549 rows affected (47.854 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 20:13:13 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 20:13:13 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://\" -e \\\n",
    "\"INSERT INTO TABLE tabla_monto_alcance (ID, Monto_Estimado, alcance) \\\n",
    "SELECT Id, MontoEstimado, Alcance FROM tabla_total;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "a60ab918-8d72-4662-875e-9b3d07529d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 9394f77e-d3aa-4d8a-92a5-c3289c9e681b\n",
      "24/11/20 20:13:18 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 20:13:18 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 20:13:19 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:13:19 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:13:19 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:13:19 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:13:19 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:13:19 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:13:19 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:13:19 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:13:19 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:13:19 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:13:19 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:13:19 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 20:13:21 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:13:21 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:13:21 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:13:21 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:13:21 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:13:21 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:13:21 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 20:13:21 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/20 20:13:24 [4f7c39ad-0c7e-4638-8d8a-9478928d1253 main]: WARN DataNucleus.Datastore: SQL Warning : Null values were eliminated from the argument of a column function.\n",
      "24/11/20 20:13:24 [4f7c39ad-0c7e-4638-8d8a-9478928d1253 main]: WARN DataNucleus.Datastore: SQL Warning : Null values were eliminated from the argument of a column function.\n",
      "24/11/20 20:13:24 [4f7c39ad-0c7e-4638-8d8a-9478928d1253 main]: WARN DataNucleus.Datastore: SQL Warning : Null values were eliminated from the argument of a column function.\n",
      "24/11/20 20:13:24 [4f7c39ad-0c7e-4638-8d8a-9478928d1253 main]: WARN optimizer.SimpleFetchOptimizer: Table default@tabla_monto_alcance is external table, falling back to filesystem scan.\n",
      "+-------------------------+-------------------------------------+------------------------------+\n",
      "| tabla_monto_alcance.id  | tabla_monto_alcance.monto_estimado  | tabla_monto_alcance.alcance  |\n",
      "+-------------------------+-------------------------------------+------------------------------+\n",
      "| 40/1-0208-LPR19         | 54000.0                             | Internacional                |\n",
      "| 14/3-0527-CDI20         | 351900.0                            | Internacional                |\n",
      "| 14/3-0525-CDI20         | 374500.0                            | Internacional                |\n",
      "+-------------------------+-------------------------------------+------------------------------+\n",
      "3 rows selected (3.648 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 20:13:25 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 20:13:25 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \"SELECT * FROM tabla_monto_alcance \\\n",
    "LIMIT 3;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ff1ce-5326-4157-b8c6-d79ddcbbd04d",
   "metadata": {},
   "source": [
    "**QUERIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "7dce7bc8-7832-49d2-ac0d-62e0bc418766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 277a688e-fd2b-425f-a87f-125218498433\n",
      "24/11/20 21:16:10 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:16:10 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:16:11 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:11 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:11 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:11 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:11 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:11 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:11 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:11 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:11 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:11 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:11 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:11 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:13 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/20 21:16:16 [10df3ad9-dcb0-4b4c-9c77-72685d703738 main]: WARN DataNucleus.Datastore: SQL Warning : Null values were eliminated from the argument of a column function.\n",
      "24/11/20 21:16:16 [10df3ad9-dcb0-4b4c-9c77-72685d703738 main]: WARN optimizer.SimpleFetchOptimizer: Table default@tabla_procedimiento_ejercicio is external table, falling back to filesystem scan.\n",
      "+-----------------------------------+------------------------------------------+---------------------------------------------------+\n",
      "| tabla_procedimiento_ejercicio.id  | tabla_procedimiento_ejercicio.ejercicio  | tabla_procedimiento_ejercicio.tipo_procedimiento  |\n",
      "+-----------------------------------+------------------------------------------+---------------------------------------------------+\n",
      "| 31-0003-CDI17                     | 2017                                     | Contratación Directa                              |\n",
      "| 37/54-0851-CDI19                  | 2019                                     | Contratación Directa                              |\n",
      "| 37/122-0993-CDI19                 | 2019                                     | Contratación Directa                              |\n",
      "| 91-0024-CDI19                     | 2019                                     | Contratación Directa                              |\n",
      "| 40/22-0969-CDI19                  | 2019                                     | Contratación Directa                              |\n",
      "+-----------------------------------+------------------------------------------+---------------------------------------------------+\n",
      "5 rows selected (3.523 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 21:16:16 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 21:16:16 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \"SELECT * FROM tabla_procedimiento_ejercicio \\\n",
    "WHERE  Tipo_Procedimiento = 'Contratación Directa' \\\n",
    "LIMIT 5;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "b869bc37-8fb0-4606-bf1a-c4d1f0f00c49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = cfb527b1-36c3-4836-840d-6c0a93e163b5\n",
      "24/11/20 21:16:35 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:16:35 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:16:36 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:36 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:36 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:36 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:36 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:36 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:36 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:36 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:36 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:36 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:36 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:36 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:16:38 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:38 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:38 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:38 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:38 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:38 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:38 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:16:38 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/20 21:16:41 [f96e2350-37cc-4d8c-8cc4-69e3667954ac main]: WARN DataNucleus.Datastore: SQL Warning : Null values were eliminated from the argument of a column function.\n",
      "24/11/20 21:16:41 [f96e2350-37cc-4d8c-8cc4-69e3667954ac main]: WARN optimizer.SimpleFetchOptimizer: Table default@tabla_procedimiento_ejercicio is external table, falling back to filesystem scan.\n",
      "+-----------------------------------+------------------------------------------+---------------------------------------------------+\n",
      "| tabla_procedimiento_ejercicio.id  | tabla_procedimiento_ejercicio.ejercicio  | tabla_procedimiento_ejercicio.tipo_procedimiento  |\n",
      "+-----------------------------------+------------------------------------------+---------------------------------------------------+\n",
      "| 31-0003-CDI17                     | 2017                                     | Contratación Directa                              |\n",
      "| 37/54-0851-CDI19                  | 2019                                     | Contratación Directa                              |\n",
      "| 37/122-0993-CDI19                 | 2019                                     | Contratación Directa                              |\n",
      "| 91-0024-CDI19                     | 2019                                     | Contratación Directa                              |\n",
      "| 40/22-0969-CDI19                  | 2019                                     | Contratación Directa                              |\n",
      "+-----------------------------------+------------------------------------------+---------------------------------------------------+\n",
      "5 rows selected (3.706 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 21:16:42 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 21:16:42 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \"SELECT * FROM tabla_procedimiento_ejercicio \\\n",
    "WHERE Tipo_Procedimiento = 'Contratación Directa' AND Ejercicio < 2020 \\\n",
    "LIMIT 5;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "3950d4aa-02c3-48df-b968-8b74edf81317",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = df53934e-b1a5-4716-aaa2-72d1f198fde3\n",
      "24/11/20 21:25:04 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:25:04 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:25:05 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:25:05 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:25:05 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:25:05 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:25:05 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:25:05 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:25:05 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:25:05 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:25:05 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:25:05 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:25:05 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:25:05 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:25:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:25:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:25:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:25:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:25:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:25:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:25:07 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:25:07 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/20 21:25:10 [HiveServer2-Background-Pool: Thread-61]: WARN ql.Driver: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez) or using Hive 1.X releases.\n",
      "Query ID = root_20241120212507_bf2a4484-de95-4275-aef6-5cbc0b65c1e9\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "24/11/20 21:25:11 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "WARN  : Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez) or using Hive 1.X releases.\n",
      "Starting Job = job_1732132153600_0015, Tracking URL = http://yarnmanager:8088/proxy/application_1732132153600_0015/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1732132153600_0015\n",
      "Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1\n",
      "24/11/20 21:25:17 [HiveServer2-Background-Pool: Thread-61]: WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead\n",
      "2024-11-20 21:25:17,531 Stage-1 map = 0%,  reduce = 0%\n",
      "2024-11-20 21:25:22,772 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 8.1 sec\n",
      "2024-11-20 21:25:27,931 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.63 sec\n",
      "MapReduce Total cumulative CPU time: 10 seconds 630 msec\n",
      "Ended Job = job_1732132153600_0015\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 10.63 sec   HDFS Read: 833280 HDFS Write: 103 HDFS EC Read: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 10 seconds 630 msec\n",
      "+---------------+\n",
      "| tipos_de_saf  |\n",
      "+---------------+\n",
      "| 125           |\n",
      "+---------------+\n",
      "1 row selected (21.839 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 21:25:29 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 21:25:29 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \"SELECT COUNT(DISTINCT(Descripcion_SAF)) AS Tipos_de_SAF FROM tabla_saf;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "7adefdbc-5b5c-433c-af41-f73c38c7f7fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = d15c3b19-fe10-4501-83e7-5a2d4709f98f\n",
      "24/11/20 21:31:42 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:31:42 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:31:43 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:31:43 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:31:43 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:31:43 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:31:43 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:31:43 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:31:43 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:31:43 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:31:43 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:31:43 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:31:43 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:31:43 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:31:44 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:31:45 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:31:45 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:31:45 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:31:45 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:31:45 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:31:45 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:31:45 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/20 21:31:48 [59cf69ad-36f7-4246-9c92-189af8293a0c main]: WARN DataNucleus.Datastore: SQL Warning : Null values were eliminated from the argument of a column function.\n",
      "24/11/20 21:31:48 [HiveServer2-Background-Pool: Thread-63]: WARN ql.Driver: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez) or using Hive 1.X releases.\n",
      "Query ID = root_20241120213145_876f9a9f-fdd1-405e-8bcd-4d5f98dac9a2\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "24/11/20 21:31:49 [HiveServer2-Background-Pool: Thread-63]: WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "WARN  : Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez) or using Hive 1.X releases.\n",
      "Starting Job = job_1732132153600_0016, Tracking URL = http://yarnmanager:8088/proxy/application_1732132153600_0016/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1732132153600_0016\n",
      "Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1\n",
      "24/11/20 21:31:55 [HiveServer2-Background-Pool: Thread-63]: WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead\n",
      "2024-11-20 21:31:55,133 Stage-1 map = 0%,  reduce = 0%\n",
      "2024-11-20 21:32:02,396 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.32 sec\n",
      "2024-11-20 21:32:07,546 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.86 sec\n",
      "MapReduce Total cumulative CPU time: 9 seconds 860 msec\n",
      "Ended Job = job_1732132153600_0016\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 9.86 sec   HDFS Read: 368155 HDFS Write: 111 HDFS EC Read: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 9 seconds 860 msec\n",
      "+--------------+\n",
      "|  max_monto   |\n",
      "+--------------+\n",
      "| 8.2881597E9  |\n",
      "+--------------+\n",
      "1 row selected (23.55 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 21:32:08 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 21:32:08 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \"SELECT MAX(Monto_Estimado) AS Max_monto FROM tabla_monto_alcance;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3bc399-c4bd-4829-8527-6d2b78bf7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_monto_alcance (ID, Monto_Estimado, alcance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "d225961f-1a8d-40c5-a2de-d75082fd10d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 32ddd6a7-69cb-4616-a28c-25c8cc05f5a8\n",
      "24/11/20 21:37:25 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:37:25 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/20 21:37:26 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:37:26 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:37:26 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:37:26 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:37:26 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:37:26 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:37:26 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:37:26 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:37:26 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:37:26 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:37:26 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:37:26 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/20 21:37:28 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:37:28 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:37:28 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:37:28 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:37:28 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:37:28 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:37:28 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/20 21:37:28 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/20 21:37:31 [8b6ff7e8-21b3-4a12-9630-9a4c0ee56c3e main]: WARN DataNucleus.Datastore: SQL Warning : Null values were eliminated from the argument of a column function.\n",
      "24/11/20 21:37:31 [HiveServer2-Background-Pool: Thread-63]: WARN ql.Driver: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez) or using Hive 1.X releases.\n",
      "Query ID = root_20241120213728_cadca6ae-004e-480a-83a7-7a0876ea658a\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "24/11/20 21:37:32 [HiveServer2-Background-Pool: Thread-63]: WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "WARN  : Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez) or using Hive 1.X releases.\n",
      "Starting Job = job_1732132153600_0018, Tracking URL = http://yarnmanager:8088/proxy/application_1732132153600_0018/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1732132153600_0018\n",
      "Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1\n",
      "24/11/20 21:37:38 [HiveServer2-Background-Pool: Thread-63]: WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead\n",
      "2024-11-20 21:37:38,518 Stage-1 map = 0%,  reduce = 0%\n",
      "2024-11-20 21:37:45,752 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.99 sec\n",
      "2024-11-20 21:37:50,889 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.61 sec\n",
      "MapReduce Total cumulative CPU time: 10 seconds 610 msec\n",
      "Ended Job = job_1732132153600_0018\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 10.61 sec   HDFS Read: 371973 HDFS Write: 157 HDFS EC Read: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 10 seconds 610 msec\n",
      "+----------------+-----------------+\n",
      "|    alcance     | monto_promedio  |\n",
      "+----------------+-----------------+\n",
      "| Internacional  | 12112060.46     |\n",
      "| Nacional       | 8836399.12      |\n",
      "+----------------+-----------------+\n",
      "2 rows selected (23.786 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/20 21:37:52 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/20 21:37:52 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \"SELECT alcance, CAST((AVG(Monto_Estimado)) AS DECIMAL(20, 2)) AS Monto_promedio FROM tabla_monto_alcance \\\n",
    "GROUP BY alcance;\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
